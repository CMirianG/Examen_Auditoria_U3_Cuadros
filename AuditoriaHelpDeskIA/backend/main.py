# backend/main.py
import sqlite3
import re
import sys 
import logging
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from typing import Literal

# --- A칌ADIDO: Importaciones para Monitorizaci칩n ---
from prometheus_fastapi_instrumentator import Instrumentator
from loguru import logger

# LangChain
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from langchain_ollama.llms import OllamaLLM
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain.chains import RetrievalQA
from langchain_core.runnables import RunnableBranch, RunnableLambda, RunnablePassthrough

# --- A칌ADIDO: CONFIGURACI칍N DE LOGGING ESTRUCTURADO ---
logger.remove()
logger.add(sys.stdout, serialize=True, enqueue=True)

class InterceptHandler(logging.Handler):
    def emit(self, record):
        try:
            level = logger.level(record.levelname).name
        except ValueError:
            level = record.levelno
        logger.log(level, record.getMessage())

logging.basicConfig(handlers=[InterceptHandler()], level=0, force=True)
logging.getLogger("uvicorn").handlers = [InterceptHandler()]
logging.getLogger("uvicorn.access").handlers = [InterceptHandler()]


# --- CONFIGURACI칍N Y MODELOS ---
VECTOR_STORE_DIR = "vector_store"
DB_PATH = "data/tickets.db"
app = FastAPI(title="Corporate EPIS Pilot API - Advanced Flow")
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"],
)

# --- A칌ADIDO: INSTRUMENTACI칍N DE PROMETHEUS ---
Instrumentator().instrument(app).expose(app)


llm = OllamaLLM(model="smollm:360m", temperature=0, base_url="http://host.docker.internal:11434")
embeddings = HuggingFaceEmbeddings(model_name="intfloat/multilingual-e5-large")
vector_store = Chroma(persist_directory=VECTOR_STORE_DIR, embedding_function=embeddings)
retriever = vector_store.as_retriever()

# --- L칍GICA DE LANGCHAIN (MODIFICADA) ---
rag_prompt_template = "Usa el siguiente contexto para responder en espa침ol de forma concisa y 칰til a la pregunta.\nContexto: {context}\nPregunta: {question}\nRespuesta:"
rag_prompt = PromptTemplate.from_template(rag_prompt_template)
rag_chain = RetrievalQA.from_chain_type(llm=llm, chain_type="stuff", retriever=retriever, chain_type_kwargs={"prompt": rag_prompt})

def create_support_ticket(description: str) -> str:
    """Crea un ticket de soporte y devuelve un mensaje de confirmaci칩n."""
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    problem_description = description.replace("ACTION_CREATE_TICKET:", "").strip()
    if not problem_description:
        problem_description = "Problema no especificado por el usuario."

    cursor.execute("INSERT INTO tickets (description, status) VALUES (?, ?)", (problem_description, "Abierto"))
    ticket_id = cursor.lastrowid
    conn.commit()
    conn.close()
    return f"De acuerdo. He creado el ticket de soporte #{ticket_id} con tu problema: '{problem_description}'. El equipo t칠cnico se pondr치 en contacto contigo."

# El router ahora es m치s simple
# CAMBIO 1: A침adimos la nueva intenci칩n 'despedida'
class RouteQuery(BaseModel):
    intent: Literal["pregunta_general", "reporte_de_problema", "despedida"] = Field(description="La intenci칩n del usuario.")

output_parser = JsonOutputParser(pydantic_object=RouteQuery)
# CAMBIO 2: Actualizamos el prompt para que el LLM sepa qu칠 es una 'despedida'
router_prompt = PromptTemplate(
    template="""Eres un clasificador de intenciones. Clasifica la pregunta del usuario en una de estas tres categor칤as:

- 'pregunta_general': El usuario pide informaci칩n (쯤u칠 es?, 쯖u치ntos?, 쯖칩mo?).
- 'reporte_de_problema': El usuario describe un problema, algo est치 roto o no funciona, necesita ayuda.
- 'despedida': El usuario expresa gratitud o se despide (gracias, adi칩s, perfecto, vale).

IMPORTANTE: Responde 칔NICAMENTE con un objeto JSON v치lido en este formato exacto:
{{"intent": "nombre_de_la_categoria"}}

Pregunta del usuario: {question}

Respuesta JSON:""",
    input_variables=["question"],
)
def extract_json_from_string(text: str) -> str:
    # Buscar el JSON m치s interno si hay m칰ltiples
    matches = list(re.finditer(r'\{[^{}]*"intent"[^{}]*\}', text, re.DOTALL))
    if matches:
        # Tomar el 칰ltimo match (m치s probable que sea el correcto)
        return matches[-1].group(0)
    
    # Si no encuentra un JSON con "intent", buscar cualquier JSON
    match = re.search(r'\{"intent":\s*"[\w_]+"\}', text, re.DOTALL)
    if match:
        return match.group(0)
    
    # Buscar cualquier JSON simple
    match = re.search(r'\{[^{}]*\}', text, re.DOTALL)
    if match:
        return match.group(0)
    
    # Si no encuentra JSON o la pregunta es muy corta, es probable que sea una despedida
    return '{"intent": "pregunta_general"}'

router_chain = router_prompt | llm | RunnableLambda(extract_json_from_string) | output_parser

chain_with_preserved_input = RunnablePassthrough.assign(decision=router_chain)

problem_chain = RunnableLambda(lambda x: {"query": x["question"]}) | rag_chain

# --- ENDPOINT DE LA API (MODIFICADO) ---
@app.get("/ask")
def ask_question(question: str):
    try:
        if question.startswith("ACTION_CREATE_TICKET:"):
            description = question.split(":", 1)[1]
            return {"answer": create_support_ticket(description), "follow_up_required": False}

        decision_result = chain_with_preserved_input.invoke({"question": question})
        intent = decision_result["decision"]["intent"]
        
        answer = ""
        follow_up = False

        if intent == "pregunta_general":
            result = problem_chain.invoke(decision_result)
            answer = result.get("result", "No se encontr칩 respuesta.")
        elif intent == "reporte_de_problema":
            result = problem_chain.invoke(decision_result)
            solution = result.get("result", "No he encontrado una soluci칩n espec칤fica en mis documentos.")
            answer = f"{solution}\n\n쮼sta informaci칩n soluciona tu problema?"
            follow_up = True
        # CAMBIO 3: A침adimos el manejo de la nueva intenci칩n
        elif intent == "despedida":
            answer = "De nada, 춰un placer ayudar! Si tienes cualquier otra consulta, aqu칤 estar칠. 游땕"
            follow_up = False
            
        return {"answer": answer, "follow_up_required": follow_up}

    except Exception as e:
        # A칌ADIDO: Usamos logger en lugar de print para un registro estructurado
        logger.error(f"Error en el endpoint /ask: {e}")
        return {"answer": "Lo siento, ha ocurrido un error.", "follow_up_required": False}